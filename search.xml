<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[randomforest应用]]></title>
      <url>%2F2018%2F01%2F08%2Frandomforest%E5%BA%94%E7%94%A8%2F</url>
      <content type="text"><![CDATA[——你好！2018！ 前话2017年有人弹了一首童年祝我生日快乐，还送来了一个拼装版的古罗哈哈哈！ 进入正题这是一个对sklearn里面随机森林算法的业务应用，主要是为了预测数据是否异常，给出正常与异常概率，这里就做一下笔记。 建模1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-import pandas as pdimport reimport csvimport pickleimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.pipeline import Pipelinerfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import classification_reportfrom sklearn.metrics import confusion_matrixfrom sklearn import metricsfilename = u'train_data.txt'data = pd.read_csv(filename, encoding='utf-8', sep='\t', error_bad_lines=False)arr_need = [u'device_change', u'area_change', u'namerule', u'narulecount' , u'urs_max_lv', u'root', u'create_channelcount', u'create_ip_ids' , u'create_ip_udid_sl', u'mail_rule']X = data.loc[:,arr_need]y = data.loc[:,'label']X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=1)pipe_lr4 = Pipeline([ ('pca', PCA(n_components='mle')), ('clf',RandomForestClassifier(random_state=0)) ])parameters4 = &#123;'clf__n_estimators':range(10,71,10),'clf__max_depth':[11, 13]&#125;clf4 = GridSearchCV(pipe_lr4, param_grid=parameters4, cv=5, scoring='accuracy')pipe_lr4.fit(X_train,y_train)print clf4.grid_scores_best_estimator = clf4.best_estimator_best_params = clf4.best_params_print best_paramsprint 'RandomForestClassifier:',pipe_lr4.score(X_test,y_test)y_pred = pipe_lr4.predict(X_test)target_names = ["1","0"]print classification_report(y_test, y_pred,target_names=target_names)print metrics.recall_score(y_test, y_pred, average='micro')print confusion_matrix(y_test, y_pred)with open('RandomForestForChuShiHao.pickle', 'wb') as fw: pickle.dump(best_estimator, fw) 预测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115# -*- coding: utf-8 -*-import pickleimport codecsimport pandasimport pandas as pdimport jsonimport redef re_match(mail): a = [] result = re.findall(ur'[a-z]+|\d+|@.+|-+|_+|\.+', mail) if result: for i in result: if re.match(ur'[a-z]+', i): a.append('C') elif re.match(ur'\d+', i): a.append('N') elif re.match(ur'@.+', i): a.append('@') elif re.match(ur'_+', i): a.append('_') elif re.match(ur'-+', i): a.append('-') elif re.match(ur'\.+', i): a.append('.') else: a.append('E') else: a.append('Q') return ''.join(a)def difference(left, right, on): """ difference of two dataframes :param left: left dataframe :param right: right dataframe :param on: join key :return: difference dataframe """ df = pd.merge(left, right, how='left', on=on) left_columns = left.columns col_y = df.columns[left_columns.size] df = df[df[col_y].isnull()] df = df.ix[:, 0:left_columns.size] df.columns = left_columns return dfdef dataformat(filename): #cur_path = os.path.abspath('.') pattern = ur'^\d&#123;11&#125;$|@qq|^m1\d&#123;10&#125;_\d+@|^m1\d&#123;10&#125;@' df_train = pd.read_csv(filename, encoding='utf-8', sep='\t', index_col=False, error_bad_lines=False) p_filter = df_train.loc[:,'aid_to_urs'].copy().str.contains(pattern) df_train = df_train[~p_filter] df_train.rename(columns = &#123;u'设备变化':u'device_change'&#125;, inplace=True) df_train.rename(columns = &#123;u'地区转移':u'area_change'&#125;, inplace=True) X = df_train.copy() X = X.loc[X['create_channel'].isin(['netease', 'netease.blued_cps_dev', 'app_store', 'uc_platform', 'bilibili_sdk'])] X.loc[:,'mail_rule'] = X.loc[:,'aid_to_urs'].map(re_match) print X.shape with codecs.open('namerule_json.json', 'r', encoding='utf-8') as f: namerule_map = json.load(f) with codecs.open('mailrule_json.json', 'r', encoding='utf-8') as f: mailrule_map = json.load(f) bool_map = &#123;False:0,True:1&#125; X = X.loc[X['namerule'].isin(namerule_map.keys())] X = X.loc[X['mail_rule'].isin(mailrule_map.keys())] X['namerule'] = X['namerule'].map(namerule_map) X['mail_rule'] = X['mail_rule'].map(mailrule_map) X['device_change'] = X['device_change'].map(bool_map) X['area_change'] = X['area_change'].map(bool_map) unknown = difference(df_train, X, 'role_id') unknown.to_csv('unknown.txt', encoding='utf-8', sep='\t', index=False) return Xdef valueTokey(value): with codecs.open('namerule_json.json', 'r', encoding='utf-8') as f: namerule_map = json.load(f) return namerule_map.keys()[namerule_map.values().index(value)]if __name__ == '__main__': filename = u'ForPredict.txt' data = dataformat(filename) arr_need = [u'device_change', u'area_change', u'namerule', u'narulecount' , u'urs_max_lv', u'root', u'create_channelcount', u'create_ip_ids' , u'create_ip_udid_sl', u'mail_rule'] X = data.loc[:,arr_need] with codecs.open('RandomForestForChuShiHao.pickle','r') as f: RandomForestForChuShiHao = pickle.load(f) # result = RandomForestForChuShiHao.predict(X) # print result y_pre = RandomForestForChuShiHao.predict_proba(X) dafr = pd.DataFrame(y_pre, columns=RandomForestForChuShiHao.classes_) data['namerule'] = data['namerule'].map(valueTokey) print dafr.head(5) data.loc[:, 'result_good'] = y_pre[:,1] data.to_csv('predict_result.txt', encoding='utf-8', sep='\t', index=False) #print data.head(3)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[社交网络分析]]></title>
      <url>%2F2017%2F11%2F20%2F%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[——好困乏。 使用networkx进行社交网络分析非常简单，一般做社交网络分析涉及的类型有： 关于用到的图论基础知识，随便google就好了，只要清楚度、边、中心性等概念即可。以下一些比较重要记录一下： GlobalClustering Coefficient：一个图中所有闭三点组的数量与所有连通三点组（无论开闭）的总量之比（也有定义为此值的三倍，使得完全图中的整体集聚系数等于1），用networkx.transitivity(G)计算。 Local Clustering Coefficient：某节点任意两个平邮彼此也是朋友的概率，用networkx.clustering(G,&#39;F&#39;)计算。 平均聚类系数：networkx.average_clustering(G) 最短路径及其节点数：networkx.shortest_path(G,&#39;A&#39;,&#39;H&#39;)、networkx.shortest_path_length(G,&#39;A&#39;,&#39;H&#39;) 中心性：Degree Centrality、Closeness Centrality、Betweenness Centrality、Page Rank]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[文本数据分析]]></title>
      <url>%2F2017%2F11%2F13%2F%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[——从未跟你饮过冰，零度天气看风景。 典型的文本预处理流程 分词：nltk.word_tokenize、nltk.sent_tokenize、jieba.cut 词形归一化：stemming：词干提取，去除ing，ed，只保留单词主干。NLTK有PoterStermmer、SnowballStemmer、LancasterStemmer。lemmatization：词形归并，将单词的各种词形归并成一种形式，NLTK有WordNetLemmatizer，并且可以指定词性。 去除停用词：可以使用nltk.corpus的stopwords，也可以自己下载停用词。 12345678910111213141516171819import nltkfrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwords# 原始文本raw_text = 'Life is like a box of chocolates. You never know what you\'re gonna get.'# 分词raw_words = nltk.word_tokenize(raw_text)# 词形归一化wordnet_lematizer = WordNetLemmatizer()words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words]# 去除停用词filtered_words = [word for word in words if word not in stopwords.words('english')]print('原始文本：', raw_text)print('预处理结果：', filtered_words) TF-IDFTF：Term Frequency(词频)，某个词在该文件中出现的次数IDF：Inverse Document Frequency(逆文档频率)，衡量某个词普遍的重要性，也有其他计算形式。TF-IDF=TF*IDF NLTK中的TF-IDF123456789101112131415from nltk.text import TextCollectiontext1 = 'I like the movie so much 'text2 = 'That is a good movie 'text3 = 'This is a great one 'text4 = 'That is a really bad movie 'text5 = 'This is a terrible movie'# 构建TextCollection对象tc = TextCollection([text1, text2, text3, text4, text5])new_text = 'That one is a good movie. This is so good!'word = 'That'tf_idf_val = tc.tf_idf(word, new_text)print('&#123;&#125;的TF-IDF值为：&#123;&#125;'.format(word, tf_idf_val)) sklearn中的TF-IDF123456789from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer()feat = vectorizer.fit_transform([text1, text2, text3, text4, text5])feature_names = vectorizer.get_feature_names()feat_array = feat.toarray()print(feature_names)print(feat_array.shape)print(vectorizer.transform([new_text]).toarray()) 主题模型及LDA12345678910111213141516import jiebaimport gensimfrom gensim import corpora, modelsch_text1 = ' 非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、妇联内部都没什么火花。团结-分裂-团结的三段式虽然老套但其实也可以利用积攒下来的形象魅力搞出意思，但剧本写得非常肤浅、平面。场面上调度混乱呆板，满屏的铁甲审美疲劳。只有笑点算得上差强人意。'ch_text2 = ' 2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出新，实则俗不可耐；以为场面很high，实则high劲不足。气！上一集的趣味全无，这集的笑点明显刻意到心虚。全片没有任何片段给我有紧张激动的时候，太弱了，跟奥创一样。'ch_text3 = ' 《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于……跟绿巨人表白了，黑寡妇用实际行动告诉了我们什么叫忠贞不二；而且为了治疗不孕不育连作战武器都变成了两支验孕棒(坚决相信快银没有死，后面还得回来)'ch_text4 = ' 虽然从头打到尾，但是真的很无聊啊。'ch_text5 = ' 剧情不如第一集好玩了，全靠密集笑点在提神。僧多粥少的直接后果就是每部寡姐都要换着队友谈恋爱，这特么比打斗还辛苦啊，真心求放过～～～（结尾彩蛋还以为是洛基呢，结果我呸！）'ch_texts = [ch_text1, ch_text2, ch_text3, ch_text4, ch_text5]doc_set = [list(jieba.cut(ch_text, cut_all=False)) for ch_text in ch_texts]dictionary = corpora.Dictionary(doc_set)corpus = [ dictionary.doc2bow(doc) for doc in doc_set[:7] ]lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)lda_model.show_topics() gesim LDA模型：https://radimrehurek.com/gensim/models/ldamodel.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[图像数据特征化处理]]></title>
      <url>%2F2017%2F10%2F30%2F%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E5%8C%96%E5%A4%84%E7%90%86%2F</url>
      <content type="text"><![CDATA[——君不见自古出征的男儿，有几个照了汗青，一个个事了拂衣去。 图像信息基础RGB颜色空间：3通道，一个像素颜色值表示为(b,g,r)，取值范围是[0, 255]，或[0.0, 1.0]，0趋向黑色。对于 单通道（黑白）图像：单个值代表的是像素值，如47就是代表303x384这个图像中的第一个像素点的颜色值。 123456789[[ 47 123 133 ..., 14 3 12] [ 93 144 145 ..., 12 7 7] [126 147 143 ..., 2 13 3] ..., [ 81 79 74 ..., 6 4 7] [ 88 82 74 ..., 5 7 8] [ 91 79 68 ..., 4 10 7]]shape:(303L, 384L) 3通道（彩色）图像：3个元素的数组代表的是像素值，如[143 120 104]就是代表300x451这个图像中的第一个像素点的颜色值。 12345678[[[143 120 104] [143 120 104] [141 118 102] ..., [ 45 27 13] [ 45 27 13] [ 45 27 13]]]shape: (300L, 451L, 3L)) 关于颜色直方图的获取 单通道（黑白）图像 123456789101112from skimage import datafrom skimage import exposure# 灰度图颜色直方图image = data.camera()print(image.shape)hist, bin_centers = exposure.histogram(image)print histplt.figure()plt.fill_between(bin_centers, hist) #填充两个函数之间的区域plt.ylim(0) 3通道（彩色）图像 1234567891011121314151617181920212223242526# 彩色图像直方图cat = data.chelsea()# R通道hist_r, bin_centers_r = exposure.histogram(cat[:,:,0])# G通道hist_g, bin_centers_g = exposure.histogram(cat[:,:,1])# B通道hist_b, bin_centers_b = exposure.histogram(cat[:,:,2])plt.figure(figsize=(10, 5))# R通道 直方图ax = plt.subplot(131)plt.fill_between(bin_centers_r, hist_r, facecolor='r')plt.ylim(0)# G通道 直方图plt.subplot(132, sharey=ax)plt.fill_between(bin_centers_g, hist_g, facecolor='g')plt.ylim(0)# B通道 直方图plt.subplot(133, sharey=ax)plt.fill_between(bin_centers_b, hist_b, facecolor='b')plt.ylim(0) 更改对比度12345# 手动：image中小于10的像素值设为0，大于180的像素值设为255high_contrast = exposure.rescale_intensity(image, in_range=(10, 180))# 自动：也叫均衡化，处理后的图像数据范围是[0, 1]equalized = exposure.equalize_hist(image) 滤波/卷积 边界补充(padding)：补零（zero-padding）、边界复制（replication）、镜像（reflection）、块复制（wraparound） 中值滤波：卷积域内的像素值取中间值作为卷积输出，有效去除椒盐噪声skimage.filters.rank.median() 高斯滤波：模拟人眼，关注中心区域，有效去除高斯噪声skimage.filters.gaussian() 均值滤波：用模板中的全体像素的平均值来代替原来像素值 图像特征描述颜色特征：颜色直方图，将256种灰度颜色分为k个区间，统计每个区间中像素总数。形状特征： SIFT(Scale-invariant feature transform),构建尺度空间→搜索尺度空间中的关键点→去除可能的噪音点→计算方向构造128维的特征向量 HOG(Histogram of Oriented Gradient) ,通过计算和统计图像局部区域的梯度方向直方图来构建特征，适合做人体检测]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sklearn机器学习二]]></title>
      <url>%2F2017%2F10%2F29%2Fsklearn%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C%2F</url>
      <content type="text"><![CDATA[——子规夜半犹啼血，不信东风唤不回。 特征工程 数值型：可直接使用，也可以进行归一化、标准化提高模型的性能sklearn.preprocessing.MinMaxScaler() 有序型：转换成数值，A–1，B–2，C–3 类别型：独热编码sklearn.preprocessing.OneHotEncoder() 123456789101112131415161718192021222324252627282930313233# 随机生成有序型特征和类别特征作为例子X_train = np.array([['male', 'low'], ['female', 'low'], ['female', 'middle'], ['male', 'low'], ['female', 'high'], ['male', 'low'], ['female', 'low'], ['female', 'high'], ['male', 'low'], ['male', 'high']])X_test = np.array([['male', 'low'], ['male', 'low'], ['female', 'middle'], ['female', 'low'], ['female', 'high']])from sklearn.preprocessing import LabelEncoder, OneHotEncoder# 在训练集上进行编码操作label_enc1 = LabelEncoder() # 首先将male, female用数字编码one_hot_enc = OneHotEncoder() # 将数字编码转换为独热编码label_enc2 = LabelEncoder() # 将low, middle, high用数字编码tr_feat1_tmp = label_enc1.fit_transform(X_train[:, 0]).reshape(-1, 1) # reshape(-1, 1)保证为一维列向量tr_feat1 = one_hot_enc.fit_transform(tr_feat1_tmp) tr_feat1 = tr_feat1.todense()tr_feat2 = label_enc2.fit_transform(X_train[:, 1]).reshape(-1, 1)X_train_enc = np.hstack((tr_feat1, tr_feat2))print(X_train_enc) 交叉验证模型的参数有两种，自身参数样本学习得到，模型自动更新，如逻辑回归、神经网络中的权重及偏置的学习；超参数手动设置的参数，如kmeans的k，神经网络中的网络层数及每层的节点个数。交叉验证就是帮助快速找到合适的超参。k-fold cross-validation,将数据分成k份，不放回每次取其中一份为测试集，其他为训练集，每次模拟得到一个准确值，然后取k次的平均值评估模型，一般会做多次k-fold cross-validation。 交叉验证：sklearn.model_selection.cross_val_score() 网格搜索：sklearn.model_selection.GridSearchCV() 单一参数123456789101112131415from sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import cross_val_scorek_range = [5, 10, 15, 20]cv_scores = []for k in k_range: knn = KNeighborsClassifier(n_neighbors=k) scores = cross_val_score(knn, X, y, cv=3) #X,y应该为全部数据，不是被split的，cv=3表示3fold cv_score = np.mean(scores) print('k=&#123;&#125;，验证集上的准确率=&#123;:.3f&#125;'.format(k, cv_score)) cv_scores.append(cv_score)best_k = k_range[np.argmax(cv_scores)]best_knn = KNeighborsClassifier(n_neighbors=best_k)best_knn.fit(X_train_scaled, y_train)print('测试集准确率：', best_knn.score(X_test_scaled, y_test)) 多个参数12345678910from sklearn.model_selection import GridSearchCVfrom sklearn.tree import DecisionTreeClassifierparameters = &#123;'max_depth':[3, 5, 7, 9], 'min_samples_leaf': [1, 2, 3, 4]&#125;clf = GridSearchCV(DecisionTreeClassifier(), parameters, cv=3, scoring='accuracy')clf.fit(X, y)print('最优参数：', clf.best_params_)print('验证集最高得分：', clf.best_score_)best_model = clf.best_estimator_print('测试集上准确率：', best_model.score(X_test, y_test)) 评价指标PredictionPositiveNegativeGround TruthPositiveTPFNNegativeFPTN TRP(Recakk,召回率)：TP/(TP+TN)Precision(精确率)：TP/(TP+FP)FPR：FP/(TN+FP) sklearn.metrics包含常用的评价指标：accuracy_score、precision_score、recall_score、f1_score PR曲线（Precision-Recall Curve）最理想的点是右上角，precision=1.0,recall=1.0,AUC的值就是曲线下的面积sklearn.metrics.precision_recall_curve() ROC曲线（Receiver Operating Chaaracteristic Curve）最理想的点是左上角，FPR=0.0，TPR=1.0sklearn.metrics.roc_curve() 混淆矩阵（confusion matrix）用于多分类模型的评价，给出的是每一类预测的准确情况sklearn.metrics.confusion_matrix() 上述主要为分类和预测的评价指标，回归模型中常用的指标有 sklearn.metrics.r2_score() sklearn.metrics.mean_absolute_error() sklearn.metrics.mean_squared_error() sklearn.metrics.median_absolute_error()更多：http://scikit-learn.org/stable/modules/model_evaluation.html 朴素贝叶斯是构建分类器的简单方法，不是训练分类器的单一算法，是一系列基于相同原理的算法，前提条件是需要假定每个特征与其他特征不相关。效率高，可用于高维数据，通常作为模型比较的baseline，但分类想过不一定好，没有超参。sklearn.naive_bayes sklearn中的三种朴素贝叶斯算法： Bernoulli：二元特征，如一个特征有没有出现 Multinomial：离散型特征，如单词出现次数 Gaussian：连续型特征 随机森林多个学习器的集成学习（Ensenle Learning），不需要过多的特征归一化和标准化 Bagging:个体学习器不存在依赖关系，如随机森林 boosting:个体学习器存在依赖关系，GBDT随机包括两部分：随机采样和随机特征，通常选取特征数为k=log2d,d为总特征数。解释 Bagging，基于自助采样法（bootsrap sampling）:有放回采样，样本在m次采样中不被采到的概率是(1-1/m)^m,取极限=1/e，约为0.368。sklearn.ensemble.RandomForestClassifier 重要参数: n_estimators:包含决策树的个数 max_features:默认即可，可调整 max_depth:每棵决策树的深度 GBDT(Gradient Boosted(-ing) Descision Tree)传统boosting关注的是先前基学习器做错的训练样本，然后基于调整后的样本分布来训练下一个基学习器，直到基学习器的数目达到预设的T值，最终将这个T个基学习器进行加权结合；而Gradient Boost是框架，可以嵌入不同的模型，迭代的方式是为了减少上一次的残差，在减少残差的梯度方向上建立新的模型，也就是每次建立的新树是前面所有树的结论和残差。不适合文本和高维数据。sklearn.ensemble.GradientBoostingClassifier 重要参数: n_estimators:包含决策树的个数 learning_rate:学习率，控制从上一次迭代中纠错的强度 max_depth:大多数应用中设置为3-5]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sklearn机器学习一]]></title>
      <url>%2F2017%2F10%2F22%2Fsklearn%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%2F</url>
      <content type="text"><![CDATA[——世间草木都美，人不是；中药很苦，你也是。 KNN(k-NearestNeighbor),k-近邻算法计算测试样本和所有训练样本的距离，选择最近的k个训练样本，统计k个训练样本的分类。 12345678910111213141516171819202122232425262728293031import numpy as npimport pandas as pdimport seaborn as snsfrom sklearn.model_selection import train_test_split# 加载数据集fruits_df = pd.read_csv('fruit_data_with_colors.txt', sep='\t', encoding='utf-8')fruits_df.head()# 划分数据集X = fruits_df[['mass', 'width', 'height', 'color_score']]y = fruits_df['fruit_label']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)# 查看数据集 sns.pairplot(data=fruits_df, hue='fruit_name', vars=['mass', 'width', 'height', 'color_score'])# 选择模型 from sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier(n_neighbors=5)# 训练模型knn.fit(X_train, y_train)# 预测模型y_pred = knn.predict(X_test)print(y_pred)# 效果from sklearn.metrics import accuracy_scoreacc = accuracy_score(y_test, y_pred) 线性回归给定样本，通过加权求和该样本的特征值计算其结果并预测求参方法：最小二乘法，即选择参数，使预测值和观测值（真实值）的平方和最小，该值称为残差平方和（residual sum of squares,RSS） 123456789101112131415161718from sklearn.datasets import make_regressionX_R1, y_R1 = make_regression(n_samples = 100, n_features=1, n_informative=1, bias = 150.0, noise = 30, random_state=0)from sklearn.linear_model import LinearRegressionX_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)# 调用线型回归模型linreg = LinearRegression()# 训练模型linreg.fit(X_train, y_train)# 输出结果print('线型模型的系数(w): &#123;&#125;'.format(linreg.coef_))print('线型模型的常数项(b): &#123;:.3f&#125;'.format(linreg.intercept_))print('训练集中R-squared得分: &#123;:.3f&#125;'.format(linreg.score(X_train, y_train)))print('测试集中R-squared得分: &#123;:.3f&#125;'.format(linreg.score(X_test, y_test))) 逻辑回归将线性回归函数取对数，引入正则项，使回归结果可以用概率解释。在sklearn中，logsitic regression的参数C使正则项系数的倒数，C=1/ λ，C小， λ大，正则化越弱，尽可能拟合训练样本数据，容易过拟合。1234567891011121314151617181920212223242526272829303132import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.linear_model import LogisticRegression# 加载数据集fruits_df = pd.read_table('fruit_data_with_colors.txt')X = fruits_df[['width', 'height']]y = fruits_df['fruit_label'].copy()# 将不是apple的标签设为0y[y != 1] = 0# 分割数据集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)# 不同的C值c_values = [0.1, 1, 100]for c_value in c_values: # 建立模型 lr_model = LogisticRegression(C=c_value) # 训练模型 lr_model.fit(X_train, y_train) # 验证模型 y_pred = lr_model.predict(X_test) acc = accuracy_score(y_test, y_pred) print('C=&#123;&#125;，准确率：&#123;:.3f&#125;'.format(c_value, acc)) 线性SVM当线性分类器有多种选择时，在样本中能够达到最大间隔的线性分类器称为线性SVM（Linear Support Vector Machine）在sklearn中，logsitic regression的参数C和论文的lambda一致，C大，正则化弱，容易过拟合。from sklearn.svm import SVC 决策树pruing:http://www.saedsayad.com/decision_tree_overfitting.htm sklearn决策树重要参数： max_depth：树的最大深度（分割点个数） min_samples_leaf：（每个叶子拥有的最少样本个数） max_leaf_modes：叶子最大个数 123456789101112131415161718192021from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitiris = load_iris()X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)max_depth_values = [2, 3, 4]for max_depth_val in max_depth_values: dt_model = DecisionTreeClassifier(max_depth=max_depth_val) dt_model.fit(X_train, y_train) print('max_depth=', max_depth_val) print('训练集上的准确率: &#123;:.3f&#125;'.format(dt_model.score(X_train, y_train))) print('测试集的准确率: &#123;:.3f&#125;'.format(dt_model.score(X_test, y_test)))# 特征的贡献值print (iris.feature_names)print (dt_model.feature_importances_)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在windows上搭建superset]]></title>
      <url>%2F2017%2F10%2F15%2F%E5%9C%A8windows%E4%B8%8A%E6%90%AD%E5%BB%BAsuperset%2F</url>
      <content type="text"><![CDATA[——三尺青锋怀天下，一骑白马开吴疆。 安装文档：http://superset.apache.org/installation.html 创建虚拟环境pip install superset 激活虚拟环境superset\Scripts\activate superset 初始化的事情基本与文档一致，只是需要切换到envs\superset\Scripts下，将所有superset替换成python superset1234567891011121314151617181920# Install supersetpip install superset# Create an admin user (you will be prompted to set username, first and last name before setting a password)fabmanager create-admin --app superset# Initialize the databasepython superset db upgrade# Load some data to play withpython superset load_examples# Create default roles and permissionspython superset init# Start the web server on port 8088, use -p to bind to another portpython superset runserver# To start a development web server, use the -d switch# python superset runserver -d 连接数据库进行可视化打开http://localhost:8088，输入原先设置好的账号密码即可登陆。登陆后打开sources-databases，新建一个数据库连接，如下图修改相关信息即可，之后就可以使用sql提取数据并进行可视化了。 相关问题 sasl.h找不到：下载合适版本的whl文件手动安装http://www.lfd.uci.edu/~gohlke/pythonlibs/#sasl。 “module” object has no attribute ‘SIGALRM‘错误：windows下依赖包不兼容，把signal所在行都注释，下面再加一个pass就好了，文件在superset/utils.py。 localhost:8088无法打开：可能是所处环境设置了代理，使用火狐浏览器设置无代理模式或者使用内网ip进行访问。 由于 gunicorn 不支持 Windows，所以只能运行开发环境。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[文本相似度]]></title>
      <url>%2F2017%2F07%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%2F</url>
      <content type="text"><![CDATA[——小本本上八卦羞答答,人生太复杂 研究什么 最长公共子串 最长公共子序列 最少编辑距离法 汉明距离 余弦相似度 那就开始最长公共子串(The Longest Common Substring)根据这个blog简单整理一下用一个矩阵来记录两个字符串中所有位置的两个字符之间的匹配情况，若是匹配则为1,否则为0。然后求出对角线最长的1的序列，其对应的位置就是最长匹配子串的位置。123456789101112def find_lcsubstr(s1, s2): m=[[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)] #生成0矩阵，为方便后续计算，比字符串长度多了一列 mmax=0 #最长匹配的长度 p=0 #最长匹配对应在s1中的最后一位 for i in range(len(s1)): for j in range(len(s2)): if s1[i]==s2[j]: m[i+1][j+1]=m[i][j]+1 if m[i+1][j+1]&gt;mmax: mmax=m[i+1][j+1] p=i+1 return s1[p-mmax:p],mmax #返回最长子串及其长度 12In [7]: print find_lcsubstr('abcdfg','abdfg')('dfg', 3) 最长公共子序列同样是根据这个blog简单整理一下子串要求字符必须是连续的，但是子序列可以不连续，用动态规划的思想：一个矩阵记录两个字符串中匹配情况，若是匹配则为左上方的值加1，否则为左方和上方的最大值。一个矩阵记录转移方向，然后根据转移方向，回溯找到最长子序列。 123456789101112131415161718192021222324252627282930313233import numpy def find_lcseque(s1, s2): # 生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果 m = [ [ 0 for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] # d用来记录转移方向 d = [ [ None for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] for p1 in range(len(s1)): for p2 in range(len(s2)): if s1[p1] == s2[p2]: #字符匹配成功，则该位置的值为左上方的值加1 m[p1+1][p2+1] = m[p1][p2]+1 d[p1+1][p2+1] = 'ok' elif m[p1+1][p2] &gt; m[p1][p2+1]: #左值大于上值，则该位置的值为左值，并标记回溯时的方向 m[p1+1][p2+1] = m[p1+1][p2] d[p1+1][p2+1] = 'left' else: #上值大于左值，则该位置的值为上值，并标记方向up m[p1+1][p2+1] = m[p1][p2+1] d[p1+1][p2+1] = 'up' (p1, p2) = (len(s1), len(s2)) print numpy.array(d) s = [] while m[p1][p2]: #不为None时 c = d[p1][p2] if c == 'ok': #匹配成功，插入该字符，并向左上角找下一个 s.append(s1[p1-1]) p1-=1 p2-=1 if c =='left': #根据标记，向左找下一个 p2 -= 1 if c == 'up': #根据标记，向上找下一个 p1 -= 1 s.reverse() return ''.join(s) 最少编辑距离编辑距离，又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数 同样是动态规划问题，这个blog说的很清晰实际应用中直接调用python-Levenshtein这个包实现，毕竟能不造轮子就不造轮子。模块通过pip install python-Levenshtein==0.12.0 进行安装。例如： 123456In [3]: Levenshtein.distance('fadg','fdag')Out[3]: 2In [4]: Levenshtein.ratio('fadg','fdag')Out[4]: 0.75 汉明距离From WIKIPEDIA：The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different.和Levenshtein distance最大的区别是Hamming distance只算替换不算插入删除。同样能不造轮子就不造轮子，这个函数要求两个字符串长度要相等。12In [7]: Levenshtein.hamming('fadgsa','fdadag')Out[7]: 5 余弦相似度基本公式： 拓展：修正余弦相似度与pearson相关系数 修正cosine考虑的是对item i打过分的每个user u，其打分的均值 Pearson考虑的是每个item i 的被打分的均值 写到这里，有一篇关于词袋模型、TF-IDF模型和LSI模型的blog值得看一下。就到这里吧！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[图像像素块统计（with OpenCV）]]></title>
      <url>%2F2017%2F06%2F06%2F%E5%9B%BE%E5%83%8F%E5%83%8F%E7%B4%A0%E5%9D%97%E7%BB%9F%E8%AE%A1%EF%BC%88with%20openCV%EF%BC%89%2F</url>
      <content type="text"><![CDATA[——十年热血写信仰，荣耀永不散场 只有一个问题，统计如下测试素材图中有多少个连在一起的像素块。 实现过程基本参考这篇文章简单修改，测试结果为13个像素块，原理不懂，看着看着才发现这个需求就是数细胞啊，当年还听老师讲过，我居然没有第一时间反应过来，Anyway，感谢google。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# -*- coding: utf-8 -*-import numpy as npimport imutilsimport cv2def counter(filename): counter = &#123;&#125; image_orig = cv2.imread(filename) height_orig, width_orig = image_orig.shape[:2] # output image with contours image_contours = image_orig.copy() # DETECTING BLUE AND WHITE COLONIES colors = ['red'] for color in colors: # copy of original image image_to_process = image_orig.copy() # initializes counter counter[color] = 0 # define NumPy arrays of color boundaries (GBR vectors) if color == 'red': lower = np.array([ 0, 0, 255]) upper = np.array([ 0, 0, 255]) # find the colors within the specified boundaries image_mask = cv2.inRange(image_to_process, lower, upper) # apply the mask image_res = cv2.bitwise_and(image_to_process, image_to_process, mask = image_mask) ## load the image, convert it to grayscale, and blur it slightly image_gray = cv2.cvtColor(image_res, cv2.COLOR_BGR2GRAY) image_gray = cv2.GaussianBlur(image_gray, (5, 5), 0) # perform edge detection, then perform a dilation + erosion to close gaps in between object edges image_edged = cv2.Canny(image_gray, 50, 100) image_edged = cv2.dilate(image_edged, None, iterations=1) image_edged = cv2.erode(image_edged, None, iterations=1) # find contours in the edge map cnts = cv2.findContours(image_edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cnts = cnts[0] if imutils.is_cv2() else cnts[1] # loop over the contours individually for c in cnts: # if the contour is not sufficiently large, ignore it if cv2.contourArea(c) &lt; 5: continue # compute the Convex Hull of the contour hull = cv2.convexHull(c) if color == 'red': # prints contours in red color cv2.drawContours(image_contours,[hull],0,(0,0,0),1) counter[color] += 1 #cv2.putText(image_contours, "&#123;:.0f&#125;".format(cv2.contourArea(c)), (int(hull[0][0][0]), int(hull[0][0][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 255, 255), 2) # Print the number of colonies of each color return counter[color] #print("&#123;&#125; &#123;&#125; colonies".format(counter[color],color)) # Writes the output image # cv2.imwrite(args["output"],image_contours)if __name__ == '__main__': print counter(u'test.png') 主要坑点1、 没法直接安装cv2，通过pip install opencv-python安装opencv,后import cv2成功。2、 这里最难的应该是设置合适的像素值，红色的RGB代码是（255，0，0），但是这里lower = np.array([ 0, 0, 255]),用的是GBR（叫你不认真看注释）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pyttsx 文本转语音]]></title>
      <url>%2F2017%2F03%2F12%2Fpyttsx-%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3%2F</url>
      <content type="text"><![CDATA[感觉挺有趣，不是很难，权当记录一下简单的实现方式，其他API实现可参考doc： 12345678910111213141516import pyttsxengine = pyttsx.init()# set voicevoices = engine.getProperty('voices')engine.setProperty('voice', voices[0])# set raterate = engine.getProperty('rate')engine.setProperty('rate', rate+30)text = u'中央电视台，中央电视台!'engine.say(text)# save to mp3.fileengine.speakToFile(text, r"./123.mp3")engine.runAndWait() 设置输出到文件 pyttsx的官方版本不支持在windows系统下输出到文件，通过下载github上的修改版本，在将其替代原来安装的版本,可以在python shell中,通过import pyttsx和pyttsx,找到pyttsx的位置。 安装comtypes、 win32com,并修改pyttsx里的sapi5.py文件，将comtypes的导入语句修改为：1234from comtypes.client import CreateObjectengine = CreateObject("SAPI.SpVoice")stream = CreateObject("SAPI.SpFileStream")from comtypes.gen import SpeechLib 参考 http://blog.wojiaohgl.com/archives/267]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[词云制作]]></title>
      <url>%2F2017%2F03%2F04%2F%E8%AF%8D%E4%BA%91%E5%88%B6%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[——你有最美的眼睛倒映着辰星，有最美的颜色如彩虹般绚丽 两种方式制作词云 使用wordcloud制作词云效果可以直接使用默认设置，或自定义设置（default_color）（costom_color） 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# -*- coding:utf-8 -*-import numpy as npfrom PIL import Imagefrom os import pathimport matplotlib.pyplot as pltimport randomimport osfrom wordcloud import WordCloud, STOPWORDSfont=os.path.join(os.path.dirname(__file__), "msyh.ttf")def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs): return "hsl(0, 0%%, %d%%)" % random.randint(60, 100)d = path.dirname(__file__)mask = np.array(Image.open(path.join(d, "timg.jpg")))# 词云定制图片模板text = open(u"test.txt").read().decode('utf-8')# adding movie script specific stopwordsstopwords = set(STOPWORDS)stopwords.add("int")stopwords.add("ext")wc = WordCloud(font_path=font, max_words=2000, background_color='black', mask=mask, stopwords=stopwords, margin=10, random_state=1).generate(text)# default colored imagedefault_colors = wc.to_array()plt.title(u"词云制作")plt.imshow(default_colors)plt.axis("off")wc.to_file("default_color.png")plt.show()# costom colored imageplt.figure()plt.title(u"词云制作")plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3))wc.to_file("costom_color.png")plt.axis("off")plt.show() wordcloud的中文显示需要指定font参数文件，本例使用系统字体msyh.ttf，可以直接从系统字体库中复制过来，修改后缀名为ttf即可 使用jieba、Taguljieba分词12345678910111213141516# -*- coding:utf-8 -*-import jieba.analyseimport osimport codecsd = os.path.dirname(__file__)file_name = os.path.join(d, "test.txt")with codecs.open(file_name, 'rb', encoding='utf-8', errors='ignore') as f: content = f.read()tags = jieba.analyse.extract_tags(content, topK=2000, withWeight=False, allowPOS=())# tags = jieba.analyse.extract_tags(content, topK=2000, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))outputfile = os.path.join(d, 'out.txt')with codecs.open(outputfile, 'w', encoding='utf-8', errors='ignore') as w: for i in tags: w.write(i + '\n') tagul打开tagul注册或登陆后，将jieba分词之后的结果（已经根据词频排序），import words导入，调整合适的配置和参数，输出即可。 （不过滤词性）（过滤词性） 使用的是tagul页面版，需要导入中文支持，测试中发现不支持微软雅黑，支持华文细黑。 jieba分词是否经过词性过滤，对结果有较大影响，根据实际情况调整 wordcloud支持输入整个文本，tagul支持输入排好序的关键词，两种方法的结果有比较大的差异，词云的制作比较依赖算法，孰优孰劣，权衡使用 参考 https://github.com/amueller/word_cloud https://www.shiyanlou.com/courses/756/labs/2521/document https://github.com/fxsjy/jieba/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[matplotlib]]></title>
      <url>%2F2017%2F02%2F26%2Fmatplotlib-%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
      <content type="text"><![CDATA[——愿你不惧荒唐，敢爱如初 从需求出发，第一次体验matplotlib模块。 需求描述为每位玩家生成一张图表，反映其在过去一周游戏货币来源途径的占比，以及其与人工预设的上一阶层的来源对比情况。效果如下： 生成简单的条形图参考文档内容：barchart_demo.py代码已经很接近需求，稍微修改一下，设置了不透明度参数、使用中文标注和稍微修改了一下颜色。实现效果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding:utf-8 -*-import numpy as npimport matplotlib.pyplot as pltN = 5yoursdata = (20, 35, 30, 35, 27)men_std = (2, 3, 4, 1, 2)ind = np.arange(N) # the x locations for the groupswidth = 0.35 # the width of the barsopacity = 0.4 # 不透明级别fig, ax = plt.subplots()rects1 = ax.bar(ind, yoursdata, width, color='b', alpha=opacity, yerr=men_std)target_data = (25, 32, 34, 20, 25)women_std = (3, 5, 2, 3, 3)rects2 = ax.bar(ind + width, target_data, width, color='r', alpha=opacity, yerr=women_std)# add some text for labels, title and axes ticksax.set_ylabel(u'数量(单位：万)')ax.set_title(u'金币周获得量对比图')ax.set_xticks(ind + width)ax.set_xticklabels(('way1', 'way2', 'way3', 'way4', 'way5'))ax.legend((rects1[0], rects2[0]), (u'您', u'对比群体'))def autolabel(rects): """ Attach a text label above each bar displaying its height """ for rect in rects: height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.05*height, '%d' % int(height), ha='center', va='bottom')autolabel(rects1)autolabel(rects2)plt.show() 对于ax.legend()的设置，也可以直接在ax.bar创建是赋以参数label,然后调用ax.legend()即可，不用再次传入参数： 12345rects1 = ax1.bar(index, yoursdata, bar_width, alpha=opacity, color='b', label=u'您')ax1.legend() matplotlib默认不支持在label等参数中传入中文，这里有三种解决办法，笔者直接尝试了第三种方法，成功解决问题 生成简单的饼形图(同理，比较简单，不赘述。)参考文档内容：pie_demo_features.py 两张图合并成一张参考文档内容：subplots_demo.py 只需要创建两个subplot，分别绘制bar和pie即可，以下三种创建subplot的写法效果一致。到这里，基本上已经完成了整个思路，最后可能需要通过plt.figure()对象的一些参数调节整个图的呈现效果，细节不详述。 1234567891011121314151617181920212223242526x = np.linspace(0, 2 * np.pi, 400)y = np.sin(x ** 2)plt.close('all')# Two subplots, the axes array is 1-df, axarr = plt.subplots(2, sharex=True)axarr[0].plot(x, y)axarr[0].set_title('Sharing X axis')axarr[1].scatter(x, y)# Two subplots, unpack the axes array immediatelyf, (ax1, ax2) = plt.subplots(2, 1, sharex=True) # 两行一列ax1.plot(x, y)ax1.set_title('Sharing X axis')ax2.scatter(x, y)# Two subplots, unpack the axes array immediatelyfig = plt.figure()ax1 = fig.add_subplot(211) # 两行一列第一个ax2 = fig.add_subplot(212) # 两行一列第二个ax1.plot(x, y)ax1.set_title('Sharing X axis')ax2.scatter(x, y)plt.show() 保存图片plt.show()只展示绘图界面，无法实现保存，保存命令需要用到fig.savefig()和fig.close(),在循环中调用时，如果不使用fig.close()，其效果是在上一张图的基础上继续作图。 最后，读取数据文件，循环生成图片即可，这里我使用json格式存储数据，结构如下： 1234567891011&#123;"role_id2":&#123;"selfget_top5":&#123;"way1":10,"way2":14,"way3":12,"way4":7,"way5":3&#125;,"cha_self_top5":&#123;"wayA":15,"wayB":13,"wayC":12,"wayD":7,"wayE":6&#125;,"cha_target_top5":&#123;"wayA":19,"wayB":17,"wayC":14,"wayD":8,"wayE":7&#125;&#125;,"role_id1":&#123;"selfget_top5":&#123;"way1":15,"way2":13,"way3":12,"way4":7,"way5":3&#125;,"cha_self_top5":&#123;"wayA":15,"wayB":13,"wayC":12,"wayD":7,"wayE":7&#125;,"cha_target_top5":&#123;"wayA":19,"wayB":17,"wayC":14,"wayD":8,"wayE":7&#125;&#125;&#125; 最后，未经优化，基本满足需求的脚本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# -*- coding:utf-8 -*-"""Bar chart demo with pairs of bars grouped for easy comparison."""import numpy as npimport jsonimport codecsimport matplotlib.pyplot as pltdef get_explode(selfdata): explode = [0, 0, 0, 0, 0] a = selfdata.index(max(selfdata)) explode.pop(a) explode.insert(a, 0.1) return explodedef autolabel(rects, ax): """ Attach a text label above each bar displaying its height """ for rect in rects: height = rect.get_height() ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * height, '%d' % int(height), ha='center', va='bottom')def jinbiduibitu(filename, selfdata, selftujing, yoursdata, targetdata, tujinglist, explode): N = 5 labels = selftujing sizes = selfdata men_std = (2, 3, 4, 1, 2) ind = np.arange(N) # the x locations for the groups width = 0.35 # the width of the bars opacity = 0.4 # 不透明度 fig = plt.figure() ax1 = fig.add_subplot(211) ax2 = fig.add_subplot(212) rects1 = ax1.bar(ind, yoursdata, width, color='b', alpha=opacity, yerr=men_std) women_std = (2, 3, 4, 1, 2) rects2 = ax1.bar(ind + width, targetdata, width, color='r', alpha=opacity, yerr=women_std) # add some text for labels, title and axes ticks ax1.set_ylabel(u'数量(单位：万)') ax1.set_title(u'金币周获得量对比图') ax1.set_xticks(ind + width) ax1.set_xticklabels(tujinglist) autolabel(rects1, ax1) autolabel(rects2, ax1) ax1.legend((rects1[0], rects2[0]), (u'您', u'对比群体')) piecs = ['red', 'yellow', 'Maroon', 'blue', 'Orange'] ax2.pie(sizes, colors=piecs, explode=explode, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90) ax2.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle. ax2.set_title(u'个人金币周获得量渠道占比图', x=0.5,y=1.1) fn = './barchart_demo_output/' + filename + '.png' fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.4) # 调整图间距 #plt.show() plt.savefig(fn) plt.close()if __name__ == '__main__': with codecs.open('barchart_demo_input.txt', 'r', encoding = 'utf-8', errors = 'ignore') as f: d = json.load(f) print len(d) for i in d.keys(): filename = i tujinglist = d[i]['cha_self_top5'].keys() selfdata = [] selftujing = d[i]['selfget_top5'].keys() for k in selftujing: sg = d[i]['selfget_top5'][k] selfdata.append(sg) yoursdata = [] targetdata = [] for j in tujinglist: yd = d[i]['cha_self_top5'][j] td = d[i]['cha_target_top5'][j] yoursdata.append(yd) targetdata.append(td) explode = get_explode(selfdata) jinbiduibitu(filename,selfdata, selftujing, yoursdata, targetdata, tujinglist, explode)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python多进程与多线程]]></title>
      <url>%2F2017%2F02%2F12%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[——五花马,千金裘,呼儿将出换美酒,与尔同销万古愁。 越来越觉得自己对这些东西一知半解…折腾了好久，今天整理一下我一知半解的 python多进程与多线程 毕竟每周还是应该多少有点什么值得记下来… 首先理解：进程&gt;线程（不知道可不可以这样讲），进程里至少有一个线程，线程之间共享内存空间，有其他进程占用内存空间时，需要等待内存闲置。一篇文章：关于进程和线程区别的类比：http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html 多进程1.创建多进程的：使用multiprocessing模块的Process类 12345678910111213141516from multiprocessing import Processimport osdef test(name): # os模块的getpid()方法可以获取当前进程的进程id print 'Run process %s (%s)...' % (name, os.getpid())if __name__ == '__main__': print 'Main process %s.' % os.getpid() # 创建一个进程 p = Process(target=test, args=('process1',)) p.start() # 调用start()方法，开始执行子进程 p.join() # 调用进程的join()方法，来阻塞除当前进程以外的所有进程,相当于是上锁的操作 print 'test finished!' 2.依次创建进程进行一次计算并测试效率 123456789101112131415161718192021222324252627282930313233from multiprocessing import Processimport timedef test(num): sum = 0 for i in range(0, num): sum += i return sumif __name__ == '__main__': start_time = time.time() test(10000000) test(10000000) test(10000000) last_time = time.time() - start_time print last_time # 常规写法，耗时2.85500001907s p1 = Process(target=test, args=(10000000,)) p2 = Process(target=test, args=(10000000,)) p3 = Process(target=test, args=(10000000,)) p1.start() p2.start() p3.start() p1.join() p2.join() p3.join() # 多进程写法，耗时1.36999988556s 这里开启进程时要注意一点：先把所有.start()写完，再写.join(),这样发生进程阻塞的情况要少，这样就能充分发挥利用cpu，提高执行效率。 3.使用进程池的方式创建进程 12345678910111213141516171819202122232425262728from multiprocessing import Poolimport timedef test(num): sum = 0 for i in range(0, num): sum += i return sumdef test_test(num): a = test(num) b = test(num) c = test(num) return a+b+cif __name__ == '__main__': start_time = time.time() pool = Pool(processes=3) result = pool.apply_async(test_test, args=(10000000,)) pool.close() pool.join() print result.get() # 如果不获取结果，效率会高很多 last_time = time.time() - start_time print last_time# 耗时：3.21100020409s 注意两点： 调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool。 apply和apply_async的区别：apply主进程会阻塞于函数，主进程的执行流程同单进程一样；apply_async是非阻塞的且支持结果返回后进行回调，主进程循环运行过程中不等待apply_async的返回结果，在主进程结束后，即使子进程还未返回整个程序也会退出。虽然apply_async是非阻塞的，但其返回结果的get方法却是阻塞的，如使用result.get()会阻塞主进程。对返回结果不感兴趣， 那么可以在主进程中使用pool.close与pool.join来防止主进程退出。注意join方法一定要在close或terminate之后调用。 Pool相关函数：https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.pool 4.使用Pool的map方法，进行多进程（cpu密集型任务）操作伪代码 1234567891011121314from multiprocessing import Pooldef method1(): return 被处理元素列表def method2(): 进行处理的方法if __name__ == '__mian__': pool = Pool() pool.map(method2, method1的结果) pool.close() pool.join() 多线程多线程适用于io密集型的任务，暂时只记使用multiprocessing.dummy这一种实现方法 12345678910111213141516171819202122232425262728import urllib2 from multiprocessing.dummy import Pool as ThreadPool urls = [ 'http://www.python.org', 'http://www.python.org/about/', 'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html', 'http://www.python.org/doc/', 'http://www.python.org/download/', 'http://www.python.org/getit/', 'http://www.python.org/community/', 'https://wiki.python.org/moin/', 'http://planet.python.org/', 'https://wiki.python.org/moin/LocalUserGroups', 'http://www.python.org/psf/', 'http://docs.python.org/devguide/', 'http://www.python.org/community/awards/' # etc.. ]# Make the Pool of workerspool = ThreadPool(4) # Open the urls in their own threads# and return the resultsresults = pool.map(urllib2.urlopen, urls)#close the pool and wait for the work to finish pool.close() pool.join() 总结几句： IO 密集型任务选择multiprocessing.dummy，CPU 密集型任务选择multiprocessing 关于IO 密集型和CPU 密集型：所谓IO密集型任务，是指磁盘IO、网络IO占主要的任务，计算量很小。比如请求网页、读写文件等。所谓计算密集型任务，是指CPU计算占主要的任务，CPU一直处于满负荷状态。比如在一个很大的列表中查找元素，复杂的加减乘除等。 文章参考： 一行 Python 实现并行化 – 日常多线程操作的新思路 python 中多进程以及多线程编程的总结 ——写的过程中还看到了进程之间的通信与同时操作一个文件的进程锁的相关内容，只能慢慢来咯…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python collections]]></title>
      <url>%2F2017%2F01%2F08%2Fpython-collections%2F</url>
      <content type="text"><![CDATA[具体实现方法参考文档：https://docs.python.org/3/library/collections.html container datetypes abstract Cool deque 双端序列 有得必有失 Counter 统计hashable objects 类似sql中的sum(A)…group by id OrderedDict 顺序存取的字典 None defaultdict key不存在时，赋予value默认值 实时统计玩家参与某副本的次数 namedtuple() 使tuple内的元素有自己的名称 坐标（x，y） 还有一些其他的类如ChainMap，暂时未归纳 defaultdict这个类想到的一些东西 在实时获取玩家参与某个玩家的次数时，由于玩家id唯一，一般作为key，使用dict比较方便，还可以应用json （听说有另外一个ujson更好用，慢慢看吧）格式进行交互，但是一开始没有进入统计范围的玩家，获取key时会报错，使用defaultdict可以解决这个问题 1234567891011# 普通字典In [7]: d = &#123;&#125;In [8]: d['a']---------------------------------------------------------------------------KeyError Traceback (most recent call last)&lt;ipython-input-8-169a40407b7f&gt; in &lt;module&gt;()----&gt; 1 d['a']KeyError: 'a' 1234567# defaultdictIn [4]: from collections import defaultdictIn [5]: d = defaultdict(int)In [6]: d['a']Out[6]: 0 实现hive中collect_all() 函数的函数,本来想在sqlite3上实现一下，但貌似不支持这个方法。 color count yellow 1 blue 2 yellow 3 blue 4 red 1 hive sql：1select color, collect_list(count) from test group by color difaultdict：12345678910In [19]: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]In [20]: d = defaultdict(list)In [21]: for k, v in s: ...: d[k].append(v) ...:In [22]: dOut[22]: defaultdict(list, &#123;'blue': [2, 4], 'red': [1], 'yellow': [1, 3]&#125;) 图床抽风用不了截图，就凑合着，中午没吃饭，又困又饿…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openpyxl操作excel文件]]></title>
      <url>%2F2016%2F12%2F31%2Fopenpyxl%E6%93%8D%E4%BD%9Cexcel%E6%96%87%E4%BB%B6%20%2F</url>
      <content type="text"><![CDATA[这个完全是为了熟悉hexo和markdown语法，上周末生日，来自一位朋友的工作需求 需求描述需要将目录中的每一行数据分别建立单独的工作表，应用工作表1的模板，并填入对于应的数据（红色框部分） 实现效果： 实现过程第一步：批量生成指定名称的工作表，excel可直接操作 选中“桥梁名称”列，先Alt+D，再Alt+P，插入数据透视表 在数据透视表字段，将“桥梁名称”拖拽到筛选区域 透视表“选项”，选择“显示报表筛选页”，确定 参考：http://jingyan.baidu.com/article/6fb756ec8c6703241858fbba.html 第二步：复制模板到各个工作表这里只需要复制模板，全选模板内容，按住shift全选工作表，Ctrl+V复制即可 第三步：填入数据这应该是可以直接用excel完成的，比较菜，不会，选择了python openpyxl 模块操作excel的方式参考openpyxl文档：http://openpyxl.readthedocs.io/en/default/usage.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding:utf-8 -*-from openpyxl import load_workbookwb = load_workbook(filename = u"xxx.xlsx")ws1 = wb.get_sheet_by_name(wb.get_sheet_names()[-2])print ws1.titlehangshu = ws1.get_highest_row()content = &#123;&#125;def get_content(): for i in range(2,217): if i &lt; 11: key = '00' + str(i-1) elif i &lt; 101: key = '0' + str(i-1) else: key = str(i-1) name_index = 'C' + str(i) bianhao_index = 'B' + str(i) dengji_index = 'H' + str(i) xunhao_index = 'K' + str(i) name = ws1[name_index].value keyname = key + name bianhao = ws1[bianhao_index].value dengji = ws1[dengji_index].value xunhao = ws1[xunhao_index].value content[keyname] = [bianhao, dengji, xunhao, name] return contentcontent = get_content()ll = 0for j in wb.get_sheet_names(): print j print content[j][3] wb.get_sheet_by_name(j)['B3'] = content[j][3] wb.get_sheet_by_name(j)['D3'] = content[j][0] wb.get_sheet_by_name(j)['F3'] = content[j][1] wb.get_sheet_by_name(j)['F26'] = content[j][2] ll += 1 if ll &gt; 214: break wb.save(filename = 'aaa_test1.xlsx')]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hello-world]]></title>
      <url>%2F2016%2F12%2F24%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
