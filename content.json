[{"title":"pytho多进程与多线程","date":"2017-02-12T02:39:50.000Z","path":"2017/02/12/pytho多进程与多线程/","text":"—五花马,千金裘,呼儿将出换美酒,与尔同销万古愁。 越来越觉得自己对这些东西一知半解…折腾了好久，今天整理一下我一知半解的 python多进程与多线程 毕竟每周还是应该多少有点什么值得记下来… 首先理解：进程&gt;线程（不知道可不可以这样讲），进程里至少有一个线程，线程之间共享内存空间，有其他进程占用内存空间时，需要等待内存闲置。一篇文章：关于进程和线程区别的类比：http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html 一、多进程1.创建多进程的：使用multiprocessing模块的Process类 12345678910111213141516from multiprocessing import Processimport osdef test(name): # os模块的getpid()方法可以获取当前进程的进程id print 'Run process %s (%s)...' % (name, os.getpid())if __name__ == '__main__': print 'Main process %s.' % os.getpid() # 创建一个进程 p = Process(target=test, args=('process1',)) p.start() # 调用start()方法，开始执行子进程 p.join() # 调用进程的join()方法，来阻塞除当前进程以外的所有进程,相当于是上锁的操作 print 'test finished!' 2.依次创建进程进行一次计算并测试效率 123456789101112131415161718192021222324252627282930313233from multiprocessing import Processimport timedef test(num): sum = 0 for i in range(0, num): sum += i return sumif __name__ == '__main__': start_time = time.time() test(10000000) test(10000000) test(10000000) last_time = time.time() - start_time print last_time # 常规写法，耗时2.85500001907s p1 = Process(target=test, args=(10000000,)) p2 = Process(target=test, args=(10000000,)) p3 = Process(target=test, args=(10000000,)) p1.start() p2.start() p3.start() p1.join() p2.join() p3.join() # 多进程写法，耗时1.36999988556s 这里开启进程时要注意一点：先把所有.start()写完，再写.join(),这样发生进程阻塞的情况要少，这样就能充分发挥利用cpu，提高执行效率。 3.使用进程池的方式创建进程 12345678910111213141516171819202122232425262728from multiprocessing import Poolimport timedef test(num): sum = 0 for i in range(0, num): sum += i return sumdef test_test(num): a = test(num) b = test(num) c = test(num) return a+b+cif __name__ == '__main__': start_time = time.time() pool = Pool(processes=3) result = pool.apply_async(test_test, args=(10000000,)) pool.close() pool.join() print result.get() # 如果不获取结果，效率会高很多 last_time = time.time() - start_time print last_time# 耗时：3.21100020409s 注意两点：a.调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool。b.apply和apply_async的区别：apply主进程会阻塞于函数，主进程的执行流程同单进程一样；apply_async是非阻塞的且支持结果返回后进行回调，主进程循环运行过程中不等待apply_async的返回结果，在主进程结束后，即使子进程还未返回整个程序也会退出。虽然apply_async是非阻塞的，但其返回结果的get方法却是阻塞的，如使用result.get()会阻塞主进程。对返回结果不感兴趣， 那么可以在主进程中使用pool.close与pool.join来防止主进程退出。注意join方法一定要在close或terminate之后调用。 Pool相关函数：https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.pool 4.使用Pool的map方法，进行多进程（cpu密集型任务）操作伪代码 1234567891011121314from multiprocessing import Pooldef method1(): return 被处理元素列表def method2(): 进行处理的方法if __name__ == '__mian__': pool = Pool() pool.map(method2, method1的结果) pool.close() pool.join() 二、多线程多线程适用于io密集型的任务，暂时只记使用multiprocessing.dummy这一种实现方法 12345678910111213141516171819202122232425262728import urllib2 from multiprocessing.dummy import Pool as ThreadPool urls = [ 'http://www.python.org', 'http://www.python.org/about/', 'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html', 'http://www.python.org/doc/', 'http://www.python.org/download/', 'http://www.python.org/getit/', 'http://www.python.org/community/', 'https://wiki.python.org/moin/', 'http://planet.python.org/', 'https://wiki.python.org/moin/LocalUserGroups', 'http://www.python.org/psf/', 'http://docs.python.org/devguide/', 'http://www.python.org/community/awards/' # etc.. ]# Make the Pool of workerspool = ThreadPool(4) # Open the urls in their own threads# and return the resultsresults = pool.map(urllib2.urlopen, urls)#close the pool and wait for the work to finish pool.close() pool.join() 总结几句：1、IO 密集型任务选择multiprocessing.dummy，CPU 密集型任务选择multiprocessing2、关于IO 密集型和CPU 密集型：所谓IO密集型任务，是指磁盘IO、网络IO占主要的任务，计算量很小。比如请求网页、读写文件等。所谓计算密集型任务，是指CPU计算占主要的任务，CPU一直处于满负荷状态。比如在一个很大的列表中查找元素，复杂的加减乘除等。 文章参考：1.一行 Python 实现并行化 – 日常多线程操作的新思路2.python 中多进程以及多线程编程的总结 —写的过程中还看到了进程之间的通信与同时操作一个文件的进程锁的相关内容，只能慢慢来咯…","comments":true,"tags":[{"name":"python","slug":"python","permalink":"http://blog.canying.org/tags/python/"}]},{"title":"python Collection类","date":"2017-01-08T05:18:20.000Z","path":"2017/01/08/python-Collection类/","text":"python collections 一直都躺在todolist里 具体实现方法参考文档：https://docs.python.org/3/library/collections.html container datetypes abstract Cool deque 双端序列 有得必有失 Counter 统计hashable objects 类似sql中的sum(A)…group by id OrderedDict 顺序存取的字典 None defaultdict key不存在时，赋予value默认值 实时统计玩家参与某副本的次数 namedtuple() 使tuple内的元素有自己的名称 坐标（x，y） 还有一些其他的类如ChainMap，暂时未归纳 主要说说defaultdict这个类想到的一些东西 在实时获取玩家参与某个玩家的次数时，由于玩家id唯一，一般作为key，使用dict比较方便，还可以应用json （听说有另外一个ujson更好用，慢慢看吧）格式进行交互，但是一开始没有进入统计范围的玩家，获取key时会报错，使用defaultdict可以解决这个问题 1234567891011# 普通字典In [7]: d = &#123;&#125;In [8]: d['a']---------------------------------------------------------------------------KeyError Traceback (most recent call last)&lt;ipython-input-8-169a40407b7f&gt; in &lt;module&gt;()----&gt; 1 d['a']KeyError: 'a' 1234567# defaultdictIn [4]: from collections import defaultdictIn [5]: d = defaultdict(int)In [6]: d['a']Out[6]: 0 实现hive中collect_all() 函数的函数,本来想在sqlite3上实现一下，但貌似不支持这个方法。 color count yellow 1 blue 2 yellow 3 blue 4 red 1 hive sql：1select color, collect_list(count) from test group by color difaultdict：12345678910In [19]: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]In [20]: d = defaultdict(list)In [21]: for k, v in s: ...: d[k].append(v) ...:In [22]: dOut[22]: defaultdict(list, &#123;'blue': [2, 4], 'red': [1], 'yellow': [1, 3]&#125;) 图床抽风用不了截图，就凑合着，中午没吃饭，又困又饿…","comments":true,"tags":[{"name":"python","slug":"python","permalink":"http://blog.canying.org/tags/python/"}]},{"title":"openpyxl操作excel文件","date":"2016-12-31T02:46:29.000Z","path":"2016/12/31/openpyxl操作excel文件 /","text":"这个完全是为了熟悉hexo和markdown语法，上周末生日，来自一位朋友的工作需求 需求描述需要将目录中的每一行数据分别建立单独的工作表，应用工作表1的模板，并填入对于应的数据（红色框部分） 实现效果： 实现过程第一步：批量生成指定名称的工作表，excel可直接操作 选中“桥梁名称”列，先Alt+D，再Alt+P，插入数据透视表 在数据透视表字段，将“桥梁名称”拖拽到筛选区域 透视表“选项”，选择“显示报表筛选页”，确定 参考：http://jingyan.baidu.com/article/6fb756ec8c6703241858fbba.html 第二步：复制模板到各个工作表这里只需要复制模板，全选模板内容，按住shift全选工作表，Ctrl+V复制即可 第三步：填入数据这应该是可以直接用excel完成的，比较菜，不会，选择了python openpyxl 模块操作excel的方式参考openpyxl文档：http://openpyxl.readthedocs.io/en/default/usage.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding:utf-8 -*-from openpyxl import load_workbookwb = load_workbook(filename = u\"xxx.xlsx\")ws1 = wb.get_sheet_by_name(wb.get_sheet_names()[-2])print ws1.titlehangshu = ws1.get_highest_row()content = &#123;&#125;def get_content(): for i in range(2,217): if i &lt; 11: key = '00' + str(i-1) elif i &lt; 101: key = '0' + str(i-1) else: key = str(i-1) name_index = 'C' + str(i) bianhao_index = 'B' + str(i) dengji_index = 'H' + str(i) xunhao_index = 'K' + str(i) name = ws1[name_index].value keyname = key + name bianhao = ws1[bianhao_index].value dengji = ws1[dengji_index].value xunhao = ws1[xunhao_index].value content[keyname] = [bianhao, dengji, xunhao, name] return contentcontent = get_content()ll = 0for j in wb.get_sheet_names(): print j print content[j][3] wb.get_sheet_by_name(j)['B3'] = content[j][3] wb.get_sheet_by_name(j)['D3'] = content[j][0] wb.get_sheet_by_name(j)['F3'] = content[j][1] wb.get_sheet_by_name(j)['F26'] = content[j][2] ll += 1 if ll &gt; 214: break wb.save(filename = 'aaa_test1.xlsx')","comments":true,"tags":[{"name":"python","slug":"python","permalink":"http://blog.canying.org/tags/python/"},{"name":"excel","slug":"excel","permalink":"http://blog.canying.org/tags/excel/"}]},{"title":"hello-world","date":"2016-12-24T00:37:25.491Z","path":"2016/12/24/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","comments":true,"tags":[]}]